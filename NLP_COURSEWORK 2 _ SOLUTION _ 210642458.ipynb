{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ca718f9e-ac8a-4750-a03a-239b2fb7fa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.6.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: contractions in /opt/conda/lib/python3.9/site-packages (0.0.58)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.9/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Requirement already satisfied: anyascii in /opt/conda/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: contractions in /opt/conda/lib/python3.9/site-packages (0.0.58)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.9/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Requirement already satisfied: anyascii in /opt/conda/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install nltk\n",
    "!pip install contractions\n",
    "import contractions\n",
    "import nltk\n",
    "\n",
    "import sys  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import shuffle, seed\n",
    "seed(0)\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "sw_nltk = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.util import bigrams\n",
    "%matplotlib inline\n",
    "pd.options.display.max_colwidth=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5391671b-a791-4b2b-8f70-f5efd6ccda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in training data and display in pandas dataframe\n",
    "train_path='training.csv'\n",
    "all_train_data = pd.read_csv(train_path,  delimiter=\"\\t\", skip_blank_lines = True)\n",
    "test_path ='test.csv'\n",
    "test_data = pd.read_csv(test_path,  delimiter=\"\\t\", skip_blank_lines = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e825ed11-1e92-45a3-9be2-cfa3b3876666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394\n",
      "1254 140\n"
     ]
    }
   ],
   "source": [
    "epsiode_scene_column = all_train_data.Episode.astype(str) + \"-\" + all_train_data.Scene.astype(str)\n",
    "all_train_data['episode_scene'] = epsiode_scene_column\n",
    "episode_scenes = sorted(list(set([x for x in epsiode_scene_column.values]))) # set function is random, need to sort!\n",
    "\n",
    "shuffle(episode_scenes)\n",
    "\n",
    "print(len(episode_scenes))\n",
    "episode_split = int(0.9*len(episode_scenes))\n",
    "training_ep_scenes = episode_scenes[:episode_split]\n",
    "test_ep_scenes = episode_scenes[episode_split:]\n",
    "print(len(training_ep_scenes), len(test_ep_scenes))\n",
    "\n",
    "def train_or_heldout_eps(val):\n",
    "    if val in training_ep_scenes:\n",
    "        return \"training\"\n",
    "    return \"heldout\"\n",
    "\n",
    "all_train_data['train_heldout'] = all_train_data['episode_scene'].apply(train_or_heldout_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d15dfcec-0409-4606-a3a0-d45b78992bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data:  (15319, 8)\n",
      "Train set:  (13638, 8)\n",
      "Validation set:  (1681, 8)\n"
     ]
    }
   ],
   "source": [
    "print('Raw Data: ',np.shape(all_train_data))\n",
    "train_data = all_train_data[all_train_data['train_heldout']=='training']\n",
    "val_data = all_train_data[all_train_data['train_heldout']=='heldout']\n",
    "print('Train set: ',np.shape(train_data))\n",
    "print('Validation set: ',np.shape(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af7c0c6-0d98-40b8-9184-f17b8163fa88",
   "metadata": {},
   "source": [
    "# Q3. Add dialogue context data and features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1a4d8b24-a456-49d4-8234-c13c2f8d9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_character_document_from_dataframe(df, max_line_count):\n",
    "    \n",
    "    character_docs = {}\n",
    "    character_lc = {}\n",
    "    sceneinfo = ''\n",
    "    lines = set()\n",
    "    for character_line, character_name, character_scene_info, character_episode, character_scene in zip(df.Line, df.Character_name, df.Scene_info, df.Episode, df.Scene):\n",
    "        if not character_name in character_docs.keys():\n",
    "            character_docs[character_name] = \"\"\n",
    "            character_lc[character_name] = 0\n",
    "            character_scene_info = character_scene\n",
    "        if character_lc[character_name]==max_line_count:\n",
    "            continue\n",
    "        character_docs[character_name] += str(character_episode) + \" --EPISODES-- \"\n",
    "        character_docs[character_name] += str(character_scene) + \"--SCENES-- \"\n",
    "        character_docs[character_name] += str(character_scene_info) + \"--SCENE_INFOORMATION-- \"\n",
    "        character_docs[character_name] += str(character_line) + \" EOL \"  # adding an end-of-line token\n",
    "        character_lc[character_name]+=1\n",
    "    #for key,value in character_docs.items():    \n",
    "        #print(key,value)\n",
    "    print(\"lines per character\", character_lc)\n",
    "    return character_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42bb2362-4066-4c61-accf-f2e115ac2d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines per character {'SHIRLEY': 360, 'OTHER': 360, 'JACK': 360, 'RONNIE': 360, 'TANYA': 360, 'SEAN': 360, 'ROXY': 360, 'HEATHER': 360, 'MAX': 360, 'IAN': 360, 'JANE': 360, 'STACEY': 360, 'PHIL': 360, 'MINTY': 360, 'CHRISTIAN': 342, 'CLARE': 352}\n",
      "Num. Characters:  16 \n",
      "\n",
      "SHIRLEY Number of Words:  6563\n",
      "OTHER Number of Words:  5964\n",
      "JACK Number of Words:  7297\n",
      "RONNIE Number of Words:  6327\n",
      "TANYA Number of Words:  6921\n",
      "SEAN Number of Words:  6738\n",
      "ROXY Number of Words:  6615\n",
      "HEATHER Number of Words:  6881\n",
      "MAX Number of Words:  7124\n",
      "IAN Number of Words:  6947\n",
      "JANE Number of Words:  6321\n",
      "STACEY Number of Words:  6637\n",
      "PHIL Number of Words:  6440\n",
      "MINTY Number of Words:  6782\n",
      "CHRISTIAN Number of Words:  6181\n",
      "CLARE Number of Words:  6982\n",
      "total words 106720\n"
     ]
    }
   ],
   "source": [
    "train_character_docs = create_character_document_from_dataframe(train_data, max_line_count=360)\n",
    "print('Num. Characters: ',len(train_character_docs.keys()),\"\\n\")\n",
    "total_words = 0\n",
    "for name in train_character_docs.keys():\n",
    "    print(name, 'Number of Words: ',len(train_character_docs[name].split()))\n",
    "    total_words += len(train_character_docs[name].split())\n",
    "print(\"total words\", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5eeba298-872b-404a-ad1d-7430a0307f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_case(match_obj):\n",
    "    char_elem = match_obj.group(0)\n",
    "    if char_elem.isupper():\n",
    "        return char_elem.lower()\n",
    "    else:\n",
    "        return char_elem.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d7eea50e-500f-48f0-bc64-c2cbc0148eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regexStrip(theString, stripChar='\\s'):\n",
    "    theRegex = re.compile(f'^({stripChar})*|({stripChar})*$')\n",
    "    stripContext = theRegex.sub('', theString)\n",
    "    return stripContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aa50248a-1cf5-4fb9-8b69-945bb8570668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f425e19d-19d0-4bcd-a246-e13f4056e317",
   "metadata": {},
   "source": [
    "# Q1. Improve pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "994f0254-94f9-4eb2-b160-e7b412ebd087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(character_text):\n",
    "    \n",
    " \n",
    "    characters = character_text.lower() # lower case\n",
    "    \n",
    "    characters= re.sub('[^\\w\\s]','',characters) #Removing Punctuation\n",
    "\n",
    "\n",
    "    character_text = regexStrip(characters)  #remove end of line\n",
    "    sent1 = decontracted(character_text) # decompressing small words in full words\n",
    "    sent1 = contractions.fix(character_text)  \n",
    "    \n",
    "    token = word_tokenize(sent1)\n",
    "    tokens_without_stopwwords = [word for word in token if not word in sw_nltk]\n",
    "\n",
    "\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemiz_sent = []\n",
    "    tags_map = defaultdict(lambda : wn.NOUN)\n",
    "    tags_map['N'] = wn.NOUN\n",
    "    tags_map['A'] = wn.ADJ\n",
    "    tags_map['V'] = wn.VERB\n",
    "    tags_map['R'] = wn.ADV\n",
    "    for token, tag in nltk.pos_tag(tokens_without_stopwwords):\n",
    "        lemma = lemmatizer.lemmatize(token, tags_map[tag[0]])\n",
    "        lemiz_sent.append(lemma) \n",
    "    return lemiz_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d137cbbc-c426-45af-b4af-feb79c76c11f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_corpus = [(name, pre_process(doc)) for name, doc in sorted(train_character_docs.items())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5acd045a-503a-4673-beca-106f1d2e69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [name for name, doc in training_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c2b8b-1c3b-406d-be90-991b159bf78e",
   "metadata": {},
   "source": [
    "# Q2. Improve linguistic feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c205aca-13b0-4a5a-b02e-3ca1758bbe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tag import CRFTagger\n",
    "\n",
    "def get_ngrams(text, n ):\n",
    "    n_grams = ngrams(text, n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d8bb194e-1ba5-4108-9149-bfc7e2235123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string  \n",
    "    return (str1.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1d23dcf4-4e77-4cda-a7c1-c5f955c9c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature_vector_dictionary(character_doc, extra_features=[]):\n",
    "    \n",
    "    counts = Counter(character_doc)  # for now a simple count\n",
    "    counts = dict(counts)\n",
    "\n",
    "    n_bigrams = get_ngrams(character_doc, 2 )\n",
    "    new_bigrams = ['@'.join(n_bigrams) for n_bigrams in n_bigrams]\n",
    "    counts.update(Counter(new_bigrams)) \n",
    "    \n",
    "   \n",
    "    pos_tagger = nltk.pos_tag(character_doc)\n",
    "    pos_tagged = [str(w)+ \"@\" + str(t) for w, t in pos_tagger]\n",
    "    \n",
    "    # make pos_tagged_words a string\n",
    "    new_pos_tagged_words = [str(pos_tagged) for pos_tagged in pos_tagged]\n",
    "    pos_counts = Counter(new_pos_tagged_words)\n",
    "    pos_counts = dict(pos_counts)\n",
    "    counts.update(pos_counts)\n",
    "\n",
    "    # sentiment analysis\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    character_doc = ' '.join(character_doc)\n",
    "    sentiment_scores = sentiment_analyzer.polarity_scores(character_doc)\n",
    "    sentiment_scores = dict(sentiment_scores)\n",
    "    counts.update(sentiment_scores)\n",
    "    \n",
    "    for feature in extra_features:\n",
    "        counts[feature] += 1\n",
    "\n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f2044-8fc5-41ad-8c41-50ce1248f1c7",
   "metadata": {},
   "source": [
    "# Q4. Improve the vectorization method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95e0c09c-be18-470e-9bc4-0364ce462b92",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpusVectorizer = DictVectorizer() \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
    "    \n",
    "    # uses the global variable of the corpus Vectorizer to improve things\n",
    "    if fitting:\n",
    "        corpusVectorizer.fit([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
    "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
    "     # use tf-idf transformer to improve things\n",
    "    \n",
    "    if fitting:\n",
    "        tfidf_transformer.fit(doc_feature_matrix)\n",
    "    doc_feature_matrix = tfidf_transformer.transform(doc_feature_matrix)\n",
    "    \n",
    "    if fitting:\n",
    "        selector = SelectKBest(score_func=chi2, k='all')\n",
    "        selector.fit(doc_feature_matrix, train_labels)\n",
    "        doc_feature_matrix = selector.transform(doc_feature_matrix)\n",
    "    \n",
    "    #training_feature_matrix[0].toarray()\n",
    "    return doc_feature_matrix\n",
    "\n",
    "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "077eeab8-6fae-44c3-967d-b6fa53489e6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines per character {'TANYA': 40, 'MAX': 40, 'SEAN': 35, 'SHIRLEY': 40, 'OTHER': 40, 'STACEY': 40, 'RONNIE': 40, 'JACK': 40, 'PHIL': 40, 'IAN': 40, 'JANE': 40, 'ROXY': 40, 'HEATHER': 40, 'MINTY': 40, 'CHRISTIAN': 40, 'CLARE': 40}\n",
      "Num. Characters:  16 \n",
      "\n",
      "TANYA Num of Words:  763\n",
      "MAX Num of Words:  1050\n",
      "SEAN Num of Words:  636\n",
      "SHIRLEY Num of Words:  628\n",
      "OTHER Num of Words:  689\n",
      "STACEY Num of Words:  702\n",
      "RONNIE Num of Words:  764\n",
      "JACK Num of Words:  619\n",
      "PHIL Num of Words:  785\n",
      "IAN Num of Words:  789\n",
      "JANE Num of Words:  738\n",
      "ROXY Num of Words:  691\n",
      "HEATHER Num of Words:  693\n",
      "MINTY Num of Words:  753\n",
      "CHRISTIAN Num of Words:  791\n",
      "CLARE Num of Words:  699\n",
      "total words 11790\n"
     ]
    }
   ],
   "source": [
    "# get the validation data- only 40 lines used for each character\n",
    "val_character_docs = create_character_document_from_dataframe(val_data, max_line_count=40)\n",
    "print('Num. Characters: ',len(val_character_docs.keys()),\"\\n\")\n",
    "total_words = 0\n",
    "for name in val_character_docs.keys():\n",
    "    print(name, 'Num of Words: ',len(val_character_docs[name].split()))\n",
    "    total_words += len(val_character_docs[name].split())\n",
    "print(\"total words\", total_words)\n",
    "\n",
    "# create list of pairs of (character name, pre-processed character) \n",
    "val_corpus = [(name, pre_process(doc)) for name, doc in sorted(val_character_docs.items())]\n",
    "val_labels = [name for name, doc in val_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b3c3d43b-87c7-41f1-998e-d9cd20d03bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(v1, v2):\n",
    "    \n",
    "    \n",
    "    # compute cosine similarity manually\n",
    "    manual_cosine_similarity = np.dot(v1, v2)  /(norm(v1) * norm(v2))\n",
    "    \n",
    "    return manual_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e4f685c2-57ba-40fe-b4bc-8204d1128891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IR_evaluation_scores(train_feature_matrix, test_feature_matrix, train_labels, test_labels):\n",
    "    \n",
    "    rankings = []\n",
    "    all_cosine_similarities = []\n",
    "    pairwise_cosine_similarity = []\n",
    "    pairs = []\n",
    "    correct = 0\n",
    "    for i, target in enumerate(test_labels):\n",
    "        # compare the left out character against the mean\n",
    "        idx = i \n",
    "        fm_1 = test_feature_matrix.toarray()[idx]\n",
    "        all_sims = {}\n",
    "        # print(\"target:\", target)\n",
    "        for j, other in enumerate(train_labels):\n",
    "            fm_2 = train_feature_matrix.toarray()[j]\n",
    "            manual_cosine_similarity = compute_cosine_similarity(fm_1, fm_2)\n",
    "            pairs.append((target, other))\n",
    "            pairwise_cosine_similarity.append(manual_cosine_similarity)\n",
    "            if other == target:\n",
    "                all_cosine_similarities.append(manual_cosine_similarity)\n",
    "            all_sims[other] = manual_cosine_similarity\n",
    "\n",
    "            # print(target, other, manual_cosine_similarity)\n",
    "        sorted_similarities = sorted(all_sims.items(),key=lambda x:x[1],reverse=True)\n",
    "        # print(sorted_similarities)\n",
    "        ranking = {key[0]: rank for rank, key in enumerate(sorted_similarities, 1)}\n",
    "        # print(\"Ranking for target\", ranking[target])\n",
    "        if ranking[target] == 1:\n",
    "            correct += 1\n",
    "        rankings.append(ranking[target])\n",
    "        # print(\"*****\")\n",
    "    mean_rank = np.mean(rankings)\n",
    "    mean_cosine_similarity = np.mean(all_cosine_similarities)\n",
    "    accuracy = correct/len(test_labels)\n",
    "    print(\"mean rank\", np.mean(rankings))\n",
    "    print(\"mean cosine similarity\", mean_cosine_similarity)\n",
    "    print(correct, \"correct out of\", len(test_labels), \"/ accuracy:\", accuracy )\n",
    "    \n",
    "    # get a dafaframe showing all the similarity scores of training vs test docs\n",
    "    df = pd.DataFrame({'doc1': [x[0] for x in pairs], 'doc2': [x[1] for x in pairs],\n",
    "                       'similarity': pairwise_cosine_similarity})\n",
    "\n",
    "    # display characters which are most similar and least similar\n",
    "    df.loc[[df.similarity.values.argmax(), df.similarity.values.argmin()]]\n",
    "    return (mean_rank, mean_cosine_similarity, accuracy, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8cd0f804-df3b-4178-9f3e-3517625e55df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f7458ddf-cd5e-44f3-baf0-230a7f6307bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16x29182 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7082 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "96fb9d4c-bd67-4628-b18c-951f38a461b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean rank 1.5625\n",
      "mean cosine similarity 0.7417053266118963\n",
      "11 correct out of 16 / accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285f527-f2de-4dfd-9743-279125293da6",
   "metadata": {},
   "source": [
    "# Q5. Select and test the best vector representation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6da93174-e9d9-456b-98ad-e366cba773ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines per character {'SHIRLEY': 400, 'OTHER': 400, 'JACK': 400, 'RONNIE': 400, 'TANYA': 400, 'SEAN': 400, 'ROXY': 400, 'HEATHER': 400, 'MAX': 400, 'IAN': 400, 'JANE': 400, 'STACEY': 400, 'PHIL': 400, 'MINTY': 400, 'CHRISTIAN': 385, 'CLARE': 400}\n",
      "Num. Characters:  16 \n",
      "\n",
      "SHIRLEY Number of Words:  7255\n",
      "OTHER Number of Words:  6664\n",
      "JACK Number of Words:  7920\n",
      "RONNIE Number of Words:  7065\n",
      "TANYA Number of Words:  7692\n",
      "SEAN Number of Words:  7446\n",
      "ROXY Number of Words:  7279\n",
      "HEATHER Number of Words:  7563\n",
      "MAX Number of Words:  8192\n",
      "IAN Number of Words:  7794\n",
      "JANE Number of Words:  7082\n",
      "STACEY Number of Words:  7357\n",
      "PHIL Number of Words:  7222\n",
      "MINTY Number of Words:  7449\n",
      "CHRISTIAN Number of Words:  7019\n",
      "CLARE Number of Words:  7832\n",
      "total words 118831\n",
      "lines per character {'SHIRLEY': 40, 'OTHER': 40, 'HEATHER': 40, 'PHIL': 40, 'SEAN': 40, 'TANYA': 40, 'MAX': 40, 'JACK': 40, 'IAN': 40, 'JANE': 40, 'STACEY': 40, 'ROXY': 40, 'RONNIE': 40, 'CHRISTIAN': 40, 'MINTY': 40, 'CLARE': 40}\n",
      "Num. Characters:  16 \n",
      "\n",
      "SHIRLEY Number of Words:  664\n",
      "OTHER Number of Words:  759\n",
      "HEATHER Number of Words:  748\n",
      "PHIL Number of Words:  707\n",
      "SEAN Number of Words:  824\n",
      "TANYA Number of Words:  839\n",
      "MAX Number of Words:  826\n",
      "JACK Number of Words:  710\n",
      "IAN Number of Words:  808\n",
      "JANE Number of Words:  695\n",
      "STACEY Number of Words:  931\n",
      "ROXY Number of Words:  711\n",
      "RONNIE Number of Words:  707\n",
      "CHRISTIAN Number of Words:  894\n",
      "MINTY Number of Words:  744\n",
      "CLARE Number of Words:  662\n",
      "total words 12229\n",
      "mean rank 1.4375\n",
      "mean cosine similarity 0.7392321608668115\n",
      "10 correct out of 16 / accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "train_character_docs = create_character_document_from_dataframe(all_train_data, max_line_count=400)\n",
    "print('Num. Characters: ',len(train_character_docs.keys()),\"\\n\")\n",
    "total_words = 0\n",
    "for name in train_character_docs.keys():\n",
    "    print(name, 'Number of Words: ',len(train_character_docs[name].split()))\n",
    "    total_words += len(train_character_docs[name].split())\n",
    "print(\"total words\", total_words)\n",
    "\n",
    "training_corpus = [(name, pre_process(doc)) for name, doc in train_character_docs.items()]\n",
    "train_labels = [name for name, doc in training_corpus]\n",
    "\n",
    "corpusVectorizer = DictVectorizer()   # initialize a corpusVectorizor which will output sparse vectors from dicts\n",
    "\n",
    "\n",
    "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
    "\n",
    "# get the test data using 40 lines per character\n",
    "test_character_docs = create_character_document_from_dataframe(test_data, max_line_count=40)\n",
    "print('Num. Characters: ',len(test_character_docs.keys()),\"\\n\")\n",
    "total_words = 0\n",
    "for name in test_character_docs.keys():\n",
    "    print(name, 'Number of Words: ',len(test_character_docs[name].split()))\n",
    "    total_words += len(test_character_docs[name].split())\n",
    "print(\"total words\", total_words)\n",
    "\n",
    "# create list of pairs of (character name, pre-processed character) \n",
    "test_corpus = [(name, pre_process(doc)) for name, doc in test_character_docs.items()]\n",
    "test_labels = [name for name, doc in test_corpus]\n",
    "\n",
    "\n",
    "# Just transform the val_feature_matrix, don't fit\n",
    "test_feature_matrix = create_document_matrix_from_corpus(test_corpus, fitting=False)\n",
    "\n",
    "\n",
    "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, test_feature_matrix, train_labels, test_labels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ab232-753d-4ca3-8e5e-f76260a1d15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b152160-aac7-4ccf-afc8-a31561a83efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731c8ea-1f0a-44c6-952e-f7d0c1352bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
